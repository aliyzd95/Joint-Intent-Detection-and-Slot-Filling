# Joint Intent Detection and Slot Filling in Persian

## ğŸ“ Overview

This project presents a deep learning-based approach to **Intent Detection** and **Slot Filling** in Persian language utterances, specifically **banking-related** user queries and commands. We utilize a modified ParsBERT-based architecture, enhanced with CNN, Dense, and BLSTM layers, to simultaneously extract the user's main intent and label each token in the input sequence.

The project consists of a single Jupyter notebook (`.ipynb`), and all the necessary datasets and models are loaded or prepared within it.

---


## ğŸ“š Dataset Description

The dataset used in this project consists of **3,307 Persian utterances**, most of which are **banking-related** user queries and commands. Each sample contains:

- A sequence of **BIO slot labels** (e.g., `B-Bankname`, `I-Amount`, `O`)
- An **intent label** (e.g., `balance_query`, `transaction_card`, `bill_payment`)
- The original **sentence in Persian**

### ğŸ“Š Example Rows

| Slot Tags                              | Intent           | Sentence                                        |
|----------------------------------------|------------------|-------------------------------------------------|
| O O O O O                              | balance_query    | .Ø¨Ú¯Ùˆ Ú†Ù†Ø¯ Ø±ÛŒØ§Ù„ Ù¾ÙˆÙ„ Ø¯Ø§Ø±Ù…                          |
| O O B-Bankname O O O                   | balance_query    | ØªÙˆÛŒ Ú©Ø§Ø±Øª Ù…Ù„ØªÙ… Ú†Ù‚Ø¯Ø± Ù¾ÙˆÙ„ Ø¯Ø§Ø±Ù…ØŸ                   |
| O O B-Billname I-Billname O O ...      | bill_payment     | .Ù‚ØµØ¯ Ø¯Ø§Ø±Ù… Ù‚Ø¨Ø¶ Ø¢Ø¨ Ù…Ø§Ù‡ Ø¬Ø§Ø±ÛŒ Ø±Ùˆ Ù¾Ø±Ø¯Ø§Ø®Øª Ú©Ù†Ù…        |
| O O O O B-Amount I-Amount I-Amount ... | transaction_card | .Ù…Ù† Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ù…Ø¨Ù„Øº 200 Ù‡Ø²Ø§Ø± ØªÙˆÙ…Ø§Ù† Ø±Ùˆ Ú©Ø§Ø±Øª Ø¨Ù‡ Ú©Ø§Ø±Øª Ú©Ù†Ù… |



## ğŸ“š Dataset Augmentation

To improve training performance, we augmented the dataset using the ChatGPT API. Approximately 3,000 new annotated sentences were generated. The process included:

- Designing an optimized prompt for GPT-3.5-Turbo.
- Post-processing generated samples.
- Creating slot annotations for each sentence using a custom NER model.
- Manual validation of generated labels.

---

## ğŸ§  Model Architecture

We use **ParsBERT** as the base model to extract contextual word embeddings. The classifier layer of ParsBERT is not usedâ€”instead, we extract the embedding layerâ€™s output of shape `(batch_size, 50, 768)` and feed it into a hybrid architecture.

### ğŸ”¹ Key Components

- **TransformerEncoderLayer**  
  A custom transformer encoder layer for further feature enrichment.

- **Intent Detection Module**  
  Includes multiple parallel CNN layers with different filter sizes (1, 5, 7), followed by pooling and dense layers.

- **Slot Filling Module**  
  A `Bidirectional LSTM` layer processes the BERT output, followed by a dense classifier for per-token labeling.

- **Feature Fusion**  
  Outputs from both branches (CNN and BLSTM) are concatenated for joint optimization.

---

## âš™ï¸ Implementation Notes

- **Input Representation**  
  The input sequences are tokenized using the ParsBERT tokenizer and padded to a fixed length of 50 tokens.

- **Encoding Labels**  
  Token labels for slot filling are aligned with tokenized words and padded appropriately.

- **Regularization Techniques**
  - Dropout layers are applied to reduce overfitting.
  - Early stopping is used during training to ensure generalization.

---

## ğŸ“ˆ Results

Despite hardware limitations, the model achieved promising accuracy due to:

- Efficient use of feature-rich embeddings from ParsBERT.
- Customized CNN and BLSTM layers for intent and slot tasks.
- Smart dataset augmentation with GPT-generated data.

---
